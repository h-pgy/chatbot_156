from typing import Any, Generator
import streamlit as st
import requests
from config import API_URL

BACKEND_URL = f"{API_URL}/ask"

st.set_page_config(page_title="Chat-156", page_icon="üí¨", layout="centered")

st.title("üí¨ Chat-156 - Assistente de Servi√ßos da Prefeitura de SP")
st.write("Este chatbot utiliza Intelig√™ncia Artificial para responder d√∫vidas sobre os servi√ßos dispon√≠veis no Portal 156 da Prefeitura de S√£o Paulo.")


# ---------------------------
# Fun√ß√£o que consome o streaming da API
# ---------------------------
def stream_from_backend(question:str) -> Generator[str, Any, None]:
    params = {"query": question}
    with requests.get(BACKEND_URL, params=params, stream=True) as r:
        r.raise_for_status()
        for chunk in r.iter_content(chunk_size=None):
            if chunk:
                yield chunk.decode("utf-8")


# ---------------------------
# UI
# ---------------------------


# ---------------------------
# SIDEBAR
# ---------------------------
with st.sidebar:
    st.title("‚ÑπÔ∏è Sobre o projeto")

    # ---------------------------
    # Se√ß√£o 1 ‚Äî Como foi feito
    # ---------------------------
    with st.container():
        st.subheader("Como foi feito")
        st.markdown(
            """
            Este chatbot utiliza uma arquitetura **RAG** e modelos **open source** para responder d√∫vidas sobre o Portal 156.
            """
        )
        with st.expander("Clique para ver mais detalhes"):
            st.markdown(
                """
                O sistema foi desenvolvido utilizando uma arquitetura **RAG (Retrieval-Augmented Generation)**.  
                Nesse modelo, antes de gerar uma resposta, a IA busca informa√ß√µes relevantes em uma base de documentos ‚Äî no caso, os textos da Carta de Servi√ßos do Portal 156, que foram scrapeados do site e estruturados em um banco de dados vetorizado (QDRANT). Isso garante respostas mais precisas, atuais e alinhadas ao conte√∫do oficial, reduzindo alucina√ß√µes comuns em modelos puramente generativos.

                Para o processamento de dados, o projeto utiliza o **modelo open source Qwen**, respons√°vel pela gera√ß√£o das respostas, em conjunto com o **modelo SBERT** para cria√ß√£o de *embeddings*, que permitem encontrar trechos relevantes nos documentos. Essa combina√ß√£o garante rapidez na recupera√ß√£o das informa√ß√µes e qualidade na gera√ß√£o do texto final.

                Este chatbot √© um **MVP experimental**, desenvolvido como trabalho final da disciplina **‚ÄúFundamentos, Governan√ßa e Ferramentas de IA para Tomada de Decis√µes‚Äù** da **Universidade Complutense de Madrid**.

                Como todo LLM, o Chat-156 pode apresentar imprecis√µes ou erros. Recomenda-se sempre consultar os canais oficiais da Prefeitura de S√£o Paulo para informa√ß√µes definitivas.
                """
            )

    st.write("---")

    # ---------------------------
    # Se√ß√£o 2 ‚Äî C√≥digo fonte
    # ---------------------------
    with st.container():
        st.subheader("C√≥digo no GitHub")

        github_logo_url = "https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png"
        col1, col2 = st.columns([1, 4])
        with col1:
            st.image(github_logo_url, width=32)
        with col2:
            st.markdown("[Reposit√≥rio no GitHub](https://github.com/h-pgy/chatbot_156)")

        st.write("---")

    # ---------------------------
    # Se√ß√£o 3 ‚Äî LinkedIn
    # ---------------------------
    with st.container():
        st.subheader("Contato profissional")
        st.markdown("**Desenvolvido por Henrique Pougy**")

        linkedin_logo_url = "https://cdn-icons-png.flaticon.com/512/174/174857.png"

        col1, col2 = st.columns([1, 4])
        with col1:
            st.image(linkedin_logo_url, width=32)
        with col2:
            st.markdown("[Meu LinkedIn](https://www.linkedin.com/in/henrique-pougy/)")




if "history" not in st.session_state:
    st.session_state.history = []


with st.form("form"):
    question = st.text_input("Fa√ßa sua pergunta sobre os servi√ßos do 156:")
    submitted = st.form_submit_button("Enviar")

if submitted and question.strip():
    resp_obj = {"question" : question}
    placeholder = st.empty()
    st.write("### Resposta:")
    answer_placeholder = st.empty()

    with st.spinner("Aguarde enquanto o Chat-156 elabora a resposta..."):
        resp_gen = stream_from_backend(question)
        full_answer = answer_placeholder.write_stream(resp_gen, cursor='...')

    resp_obj["answer"] = full_answer

    # Armazena no hist√≥rico
    st.session_state.history.append(resp_obj)


# ---------------------------
# Mostrar hist√≥rico
# ---------------------------
if st.session_state.history:
    st.write("---")
    st.write("### Hist√≥rico de conversas")
    with st.container():
        for msg in st.session_state.history:
            with st.expander(msg['question'][:100]+'...', expanded=False):
                st.markdown(f"**Voc√™:** {msg['question']}")
                st.markdown(f"**Chat-156:** {msg['answer']}")
    # Bot√£o de limpar hist√≥rico
    if st.button("üóëÔ∏è Limpar hist√≥rico"):
        st.session_state.history = []
        st.rerun()

# ---------------------------
# Footer ‚Äî Aviso de uso respons√°vel
# ---------------------------
st.divider()
st.markdown(
    """
### üõ°Ô∏è Aviso de uso respons√°vel da IA
As respostas s√£o geradas por modelos de IA e **podem conter imprecis√µes**.  
N√£o substituem canais oficiais da Prefeitura de S√£o Paulo.
    """
)




